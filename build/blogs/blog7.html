
<!DOCTYPE html><html lang="en">
<head>
<title>Sam Ly</title>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css" />
<link rel="stylesheet" href="/style.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script><script src="/index.js"></script></head><body><main>
<h1>
Information Theory
</h1>
<p>
<em>Credits: Deep Learning, Ian Goodfellow</em>

</p>
<blockquote><p>
Learning that an unlikely event has occurred is more informative than learning that a likely event has occurred.
</p></blockquote>
<p>
Imagine someone told you that the sun rose this morning. This message contains zero information as we consider the event of the sun rising each to have a probability of 1. Now consider that someone else told you that there was a solar eclipse this morning. The event of a solar eclipse is relatively unlikely, thus learning about this event happening is rather informative.
</p>
<p>
To quantify this, we can use the units of 
<strong>shannons</strong>
 or 
<strong>nats</strong>
. They measure the same thing, but are scaled by a constant factor.
</p>
<p>
We define the 
<strong>self-information</strong>
 of an event 
<em>x</em>
 as the negative log of the probability of 
<em>x</em>
.
</p>
<p>
<em>I(x) = -log P(x)</em>

</p>
<p>
Choosing a log with base 2 gives an result in shannons, and choosing the natural log gives a result in terms of nats.
</p>
<p>
Now, taking the weighted average of the self-information gives you the 
<strong>Shannon entropy</strong>
, a measure of the expected amount of uncertainty in an entire probability distribution.
</p>
<p>
<em>H(x) = E(I(X))</em>

</p>
<h2>
Kullback-Leibler divergence
</h2>
<p>
Given two distributions over the same RV 
<em>P(x)</em>
 and 
<em>Q(x)</em>
, we can measure how different they are via the 
<strong>KL divergence</strong>
. 
</p>
<p>

<img alt="" src="https://i.imgur.com/cpYL0Jc.png" referrerpolicy="no-referrer" />

</p>
<p>
The KL divergence is not a true distance measure as D (P || Q) != D (Q || P).
</p>
<hr />
<p>
<a href="/index.html">home</a>
 | 
<a href="/contact.html">contact</a>
 | 
<a href="/blogs/index.html">blogs</a>

</p></main></body></html>