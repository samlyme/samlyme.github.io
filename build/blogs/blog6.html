
<!DOCTYPE html><html lang="en">
<head>
<title>Sam Ly</title>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css" />
<link rel="stylesheet" href="/style.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script><script src="/index.js"></script></head><body><main>
<h1>
Intermediate(ish) probability
</h1>
<p>
<em>Credits: Deep Learning, Ian Goodfellow</em>

</p>
<h2>
3.9.6 Mixtures of Distributions
</h2>
<p>
An easy way to create a new, more complex, distribution is to mix existing distributions. Imagine we have 
<em>n</em>
 independent distributions. On each trial, we pick a specific 
<strong>component distribution</strong>
, denoted by the RV 
<em>c</em>
. We then use distribution 
<em>c</em>
 to generate an RV 
<em>x</em>
. 
</p>
<p>
The probability of picking a specific distribution 
<em>ci</em>
 is modeled by a 
<strong>multinoulli distribution</strong>
. We do this so we can weight certain distributions more heavily.
</p>
<p>
Now, the 
<em>overall</em>
 distribution is modeled by:
</p>
<p>
<em>P(x) = sum( P(c = i) P(x | c = i))</em>

</p>
<blockquote><p>
This can also be interpreted as the marginal probability of 
<em>x</em>

</p></blockquote>
<p>
Now, imagine we don&#39;t know how to pick the the distribution 
<em>c</em>
. In other words 
<em>P(c = i)</em>
 is unknown. This makes 
<em>c</em>
 a 
<strong>latent variable</strong>
, meaning it is a random variable that we cannot observe directly. However, this variable still has a measurable impact on the outcome of our system.
</p>
<p>
Imagine this contrived situation: 
<br />
We have a bag of red and blue marbles. We know (somehow) that Factory 1 produces 80% red and 20% blue, and that Factory 2 produces 30% red and 70% blue. However, we don&#39;t know how many marbles come from each factory.
</p>
<p>
In this scenario, our latent variable 
<em>c</em>
 represents which factory the marble came from. 
</p>
<p>
Our overall goal is to predict the distribution of marbles in the bag 
<em>P(x)</em>
. 
</p>
<p>
Intuitively, we know that if the bag has mostly red marbles, most of the bag came from Factory 1, and vice versa. Our model 
<strong>learns</strong>
 by guessing the values of 
<em>P(c)</em>
.
</p>
<p>
In this situation, it is clear that the latent variable is unnecessary in modeling the distribution of marbles, but it does allow for us to 
<strong>interpret</strong>
 why the model is making the predictions that it is. eg. The model is predicting an 80% chance of red because it thinks all the marbles come from Factory 1.
</p>
<p>
Formally speaking, we have:
</p>
<p>
Assumptions about the system:
<br />
<em>P(x = red | c = Factory 1) = 0.8</em>
 -&gt; Bernoulli Distribution
<br />
<em>P(x = red | c = Factory 2) = 0.3</em>
 -&gt; Bernoulli Distribution
</p>
<p>
<strong>Prior probability: </strong>

<br />
<em>a = P(c = i)</em>
 -&gt; Trying to guess the latent variable 
<em>c</em>

</p>
<p>
<strong>Posterior probability: </strong>

<br />
<em>P(c | x)</em>
 -&gt; What is the distribution of 
<em>c</em>
 give our measurement 
<em>x</em>
?
</p>
<blockquote><p>
Intuition tells us that if we get a measurement 
<em>x</em>
 = Blue, we are more likely to have more marbles coming from Factory 2.
</p></blockquote>
<h3>
trust me bro type theorem
</h3>
<blockquote><p>
&quot;A Gaussian mixture model is a 
<strong>universal approximator</strong>
 of densities, in the sense that any smooth density can be approximated with any specific nonzero amount of error by a Gaussian mixture model with enough components.&quot;
</p></blockquote>
<h2>
Expectation-maximization
</h2>
<p>
Some python code to solve for the latent variable:
</p>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
P(x = red | c = Factory 1) = 0.8 -&gt; Bernoulli Distribution
P(x = red | c = Factory 2) = 0.3 -&gt; Bernoulli Distribution
&quot;&quot;&quot;
p_red_given_f1 = 0.8
p_red_given_f2 = 0.3

&quot;&quot;&quot;
True proportion from Factory 1
P(c = 1) = 0.6
&quot;&quot;&quot;
p_f1 = 0.6

print(&quot;Ground truth&quot;)
print(f&quot;P(c = Factory 1) = {p_f1:.2f}&quot;)
print(f&quot;P(c = Factory 2) = {1 - p_f1:.2f}&quot;)
print(f&quot;P(x = red | c = Factory 1) = {p_red_given_f1:.2f}&quot;)
print(f&quot;P(x = red | c = Factory 2) = {p_red_given_f2:.2f}&quot;)

K = 1000

C = np.random.binomial(1, p_f1, size=K)

# 1 = red, 0 = blue
X = np.where(C  == 1, 
             np.random.binomial(1, p_red_given_f1, size=K), 
             np.random.binomial(1, p_red_given_f2, size=K))

print(f&quot;Generate {K} samples:&quot;)
x = np.sum(X)
print(&quot;Red: &quot;, x)
print(&quot;Blue:&quot;, K-x)
print(f&quot;Observed P(x = red): {x/K}&quot;)

# Initial random guess of P(c = 1) = 0.5
p_hat_f1 = 0.5

max_iterations = 100
tolerance = 10e-9

for iteration in range(max_iterations):
    p_x_c1 = np.where(X == 1, 
                             p_red_given_f1, 
                             (1 - p_red_given_f1)) * p_hat_f1
    p_x_c2 = np.where(X == 1, 
                             p_red_given_f2, 
                             (1 - p_red_given_f2)) * (1-p_hat_f1)
    # probability of x, with our assumptions of p_hat_f1
    p_x = p_x_c1 + p_x_c2

    # Gamma, the responsibilities ??
    gamma_c1 = p_x_c1 / p_x
    gamma_c2 = p_x_c2 / p_x

    p_hat_f1p = np.sum(gamma_c1) / K

    if np.abs(p_hat_f1p - p_hat_f1) &lt; tolerance:
        print(f&quot;Converged at iteration {iteration}&quot;)
        break

    p_hat_f1 = p_hat_f1p
    
    # show progress
    if iteration % 10 == 0:
        print(f&quot;{iteration}: P(c = Factory 1) = {p_hat_f1}&quot;)
        
print(f&quot;Final solution: P(c = Factory 1) = {p_hat_f1:.2f}&quot;)
    
</code></pre>
<p>
<em>Note:</em>
 This can be generalized to 
<em>n</em>
 arbitrary component distributions. This is left as an exercise for the reader.
</p></main></body></html>